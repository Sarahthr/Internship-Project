{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84cf2ded-3c58-4b41-af2b-a56b09ab65b8",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1300e670-5735-4f92-b3dd-a0d099291c67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Utilities\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Parallel processing\n",
    "import concurrent.futures\n",
    "\n",
    "# Google Cloud Storage\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd334e9d-782f-4b76-a686-4a07e3d82956",
   "metadata": {},
   "source": [
    "## File Classification and Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a04036bf-6c96-4afc-b271-a64f07f8d3b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(\"all_files.json\") :\n",
    "    with open(\"all_files.json\", 'r') as f :\n",
    "        files = json.load(f)\n",
    "        # files[0]     All the files seen\n",
    "        # files[1]     File paths that were successfully read and validated with alpha and bêta angle in degree\n",
    "        # files[2]     File paths that were successfully read and validated with alpha and bêta angle in radian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47e54d29-d956-4137-8bc4-ae57631dcd62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12510, 12510)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Google Cloud Storage (GCS) client\n",
    "client = storage.Client()\n",
    "\n",
    "# Define the Google Cloud Storage folder to read files from\n",
    "gcs_path = \"featurestore-spinewise/\"\n",
    "\n",
    "# Extract the bucket name and path prefix\n",
    "bucket_name = gcs_path.split(\"/\")[0]\n",
    "prefix = \"/\".join(gcs_path.split(\"/\")[1:])\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "# List all files available in the GCS path\n",
    "paths = [f\"gs://{bucket_name}/{blob.name}\" for blob in bucket.list_blobs(prefix=prefix)]\n",
    "\n",
    "filePaths_id = []  # List of unique file names (used to avoid duplicates)\n",
    "filePaths = []  # List of corresponding full paths (gs://.../file.parquet)\n",
    "\n",
    "# Keep only one path per file name (each file name is unique even if stored in multiple folders)\n",
    "for file in paths :\n",
    "    if file.endswith('.parquet') :\n",
    "        file_n = file.split(\"/\")[-1]\n",
    "        if file_n not in filePaths_id :\n",
    "            filePaths_id.append(file_n)\n",
    "            filePaths.append(file)\n",
    "\n",
    "# Display stats : (files to process in GCS, files already processed)\n",
    "len(filePaths), len(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2a77f15-5f03-4d68-a723-805dc40ffb74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12510, 12510)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt to read a parquet file and check it has the required columns and enough data\n",
    "def read_check_parquet(i, path):\n",
    "    print(i)\n",
    "    df = pd.read_parquet(path)\n",
    "    # Check required columns and minimum length\n",
    "    if not {\"alpha_angle\", \"beta_angle\"}.issubset(df.columns) or len(df) < 11250 :\n",
    "        return None\n",
    "    return df\n",
    "\n",
    "id_file = [file.split(\"/\")[-1] for file in files[0]] # List of already seen file names (used to skip duplicates)\n",
    "\n",
    "# Loop through all new files to validate and classify them\n",
    "for i in range(len(filePaths)):\n",
    "    if filePaths[i].split(\"/\")[-1] not in id_file :\n",
    "        \n",
    "        # Use a thread with timeout to avoid long-loading files\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor :\n",
    "            future = executor.submit(read_check_parquet, i, filePaths[i])\n",
    "            try :\n",
    "                res = future.result(timeout=15) # If the file loading exceed 15 sec then return None\n",
    "                if res is not None :\n",
    "                    max_a, min_a, max_b, min_b = max(res[\"alpha_angle\"]), min(res[\"alpha_angle\"]), max(res[\"beta_angle\"]), min(res[\"beta_angle\"])\n",
    "                    if max_a > 5 or min_a < -2 or max_b > 5 or min_b < -2 : # Because here radian_angle ∈ [ 3*π/2 ; π/2 ]\n",
    "                        files[1].append(filePaths[i]) # Add to degree folder\n",
    "                    else : \n",
    "                        files[2].append(filePaths[i]) # Add to radian folder\n",
    "                        \n",
    "            except concurrent.futures.TimeoutError :\n",
    "                print(f\"File {i} exceeded loading time (>15s), skipped\")\n",
    "\n",
    "        # Append the file to the global list of seen files\n",
    "        files[0].append(filePaths[i])\n",
    "        id_file.append(filePaths[i].split(\"/\")[-1])\n",
    "        \n",
    "# Final save of the file list after processing\n",
    "with open(\"all_files.json\", \"w\") as f:\n",
    "    json.dump(files, f, indent=4)\n",
    "\n",
    "len(filePaths), len(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d159f35-4545-4961-8ce9-f6a2e82a5364",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12510, 12510)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It's possible to have more files in the JSON than in the GCS (if some files were removed from GCS)\n",
    "file0, file1, file2 = [], [], []\n",
    "\n",
    "for f in files[0] :\n",
    "    if f in filePaths :\n",
    "        file0.append(f)\n",
    "\n",
    "for f in files[1] :\n",
    "    if f in filePaths :\n",
    "        file1.append(f)\n",
    "        \n",
    "for f in files[2] :\n",
    "    if f in filePaths :\n",
    "        file2.append(f)\n",
    "\n",
    "with open(\"all_files.json\", \"w\") as f:\n",
    "    json.dump(files, f, indent=4)\n",
    "    \n",
    "len(filePaths), len(files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcf1100-09f5-4d77-9717-005f04fcf0df",
   "metadata": {},
   "source": [
    "## How to Save and Load Models/Scalers using joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca77cb76-1cdc-4148-8551-5305d727887f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n",
      "Scaler saved\n",
      "\n",
      "Model loaded\n",
      "Scaler loaded\n",
      "\n",
      "Example predictions : [1 0 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# === Step 1 : Train a model and fit a scaler ===\n",
    "\n",
    "# For exemple we take Iris dataset\n",
    "X, Y = load_iris(return_X_y=True)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# === Step 2 : Save the model and the scaler ===\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"rd_iris_model.joblib\")\n",
    "print(\"Model saved\")\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, \"scaler.joblib\")\n",
    "print(\"Scaler saved\\n\")\n",
    "\n",
    "# === Step 3: Load the model and the scaler later for prediction ===\n",
    "\n",
    "# Load model\n",
    "loaded_model = joblib.load(\"rd_iris_model.joblib\")\n",
    "print(\"Model loaded\")\n",
    "\n",
    "# Load scaler\n",
    "loaded_scaler = joblib.load(\"scaler.joblib\")\n",
    "print(\"Scaler loaded\\n\")\n",
    "\n",
    "# Use loaded scaler and model to make a prediction\n",
    "X_test_scaled = loaded_scaler.transform(X_test)\n",
    "predictions = loaded_model.predict(X_test_scaled)\n",
    "print(\"Example predictions :\", predictions[:5])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11 (Local)",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
